<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hedwig Explains</title>
    <description>A healthy mind is an inquisitive mind</description>
    <link>https://hiteshhedwig.github.io/hedwig_explains/</link>
    <atom:link href="https://hiteshhedwig.github.io/hedwig_explains/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 21 Aug 2021 12:57:40 +0530</pubDate>
    <lastBuildDate>Sat, 21 Aug 2021 12:57:40 +0530</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>[Go] A tutorial on go-rpi-rgb-led-matrix</title>
        <description>&lt;h1 id=&quot;brief-&quot;&gt;Brief :&lt;/h1&gt;

&lt;p&gt;In this blog, we will be diving in an excellent &lt;a href=&quot;https://github.com/hzeller/rpi-rgb-led-matrix&quot;&gt;github repo&lt;/a&gt;. Which allows us to control RGB LED display. I will explain, how to setup the environment and code on your machine. Basically, author provides binding that can be used seemlessly in python and C#. While, some others have used this API to create some other bindings like in go, nodejs, rust. In this tutorial, i will be using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Go&lt;/code&gt; binding as an example.&lt;/p&gt;

&lt;h1 id=&quot;go-binding-dilemma-&quot;&gt;Go binding dilemma ~&lt;/h1&gt;

&lt;p&gt;Yeah, now here. I’ve had my share bit of troubles using the &lt;a href=&quot;https://github.com/mcuadros/go-rpi-rgb-led-matrix&quot;&gt;go binding&lt;/a&gt;. It gave me plethora of pain. Somehow the suggested way to install it on a given system wasn’t working my way. AND, the repo is last updated 3 years ago. So i had my issues installing it. Fortunately, i found &lt;a href=&quot;https://github.com/RockKeeper/go-rpi-rgb-led-matrix&quot;&gt;this repo&lt;/a&gt; forked and updated. So, i used this particular repo for my project. Forked and made changes.&lt;/p&gt;

&lt;h1 id=&quot;its-installation-&quot;&gt;It’s installation ~&lt;/h1&gt;

&lt;p&gt;Although, i am a big fan of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go get &amp;lt;something&amp;gt;&lt;/code&gt; feature in go for any 3rd party package installation. In this case, i wasn’t. So, i simply cloned &lt;a href=&quot;https://github.com/RockKeeper/go-rpi-rgb-led-matrix&quot;&gt;github repo&lt;/a&gt; in my go workspace.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note : Cloning my look cumbersome errand to some but in my case, i wanted to make sure i can do some required changes. Having full control over the repo is the way how i go about.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/qz2JCrB4&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/02BbF5LM/Screenshot-from-2021-08-21-11-36-28.png&quot; alt=&quot;Screenshot-from-2021-08-21-11-36-28.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you can see, in the workspace i have only go.mod, go.sum, and sample test go file. As my IDE says, it can’t find the repo. Coz, i have installed it yet. Let’s go through and do it :&lt;/p&gt;

&lt;h2 id=&quot;cloning-the-repo-&quot;&gt;Cloning the repo :&lt;/h2&gt;

&lt;p&gt;So here’s how i cloned the repo ~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/t7MkV0SS&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/wT6GZghp/Screenshot-from-2021-08-21-11-44-21.png&quot; alt=&quot;Screenshot-from-2021-08-21-11-44-21.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And further i create a directory called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tutorial&lt;/code&gt; and put the cloned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go-rpi-rgb-led-matrix&lt;/code&gt; into it. Here’s how it looks like :&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/rK410Dts&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/sD6wLSjY/Screenshot-from-2021-08-21-11-46-05.png&quot; alt=&quot;Screenshot-from-2021-08-21-11-46-05.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now after that, change go into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go-rpi-rgb-led-matrix&lt;/code&gt; &amp;amp; delete go.mod &amp;amp; go.sum files. Further edit matrix.go file. Find this snippet in the file where file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import &quot;C&quot;&lt;/code&gt;. Edit&lt;/p&gt;

&lt;p&gt;From this :&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import &quot;C&quot;
import (
	&quot;fmt&quot;
	&quot;image/color&quot;
	&quot;os&quot;
	&quot;unsafe&quot;

	&quot;github.com/RockKeeper/go-rpi-rgb-led-matrix/emulator&quot;
)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To this :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import &quot;C&quot;
import (
	&quot;fmt&quot;
	&quot;image/color&quot;
	&quot;os&quot;
	&quot;unsafe&quot;

	&quot;rgbtest/tutorial/go-rpi-rgb-led-matrix/emulator&quot;
)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;make sure your root go.mod is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rgbtest&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now, cd into lib folder in go-rpi-rgb-led-matrix and run this,&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/ZCQz5Kyw&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/CxhhJRMp/Screenshot-from-2021-08-21-12-26-45.png&quot; alt=&quot;Screenshot-from-2021-08-21-12-26-45.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;git submodule update --init
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And after that, follow this! (i know too much steps but its worth it)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/4YrKVgj5&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/YSS1c23J/Screenshot-from-2021-08-21-12-30-30.png&quot; alt=&quot;Screenshot-from-2021-08-21-12-30-30.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When you run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make&lt;/code&gt; &amp;amp; something like this will go on:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/cKSxd4VV&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/5tH6qXpN/Screenshot-from-2021-08-21-12-30-39.png&quot; alt=&quot;Screenshot-from-2021-08-21-12-30-39.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now finally all chaos done. When you go back and run the file [I HAD CHANED &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main_rgb.go to main.go&lt;/code&gt; to maintain consistency]:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/QVH3GTFF&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/xjPTmGY3/Screenshot-from-2021-08-21-12-35-00.png&quot; alt=&quot;Screenshot-from-2021-08-21-12-35-00.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;NO! after all this? Yeah. if you look carefully, you will see its saying something like&lt;/p&gt;

&lt;h1 id=&quot;climax-&quot;&gt;Climax ~&lt;/h1&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;non-existent blah blah Could not determine Pi model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Yeah, if you run it on Raspberry Pi. It will work. But how to run it on PC?.&lt;/p&gt;

&lt;p&gt;Well fortuantely, we have been provided with an emulator to solve our case.&lt;/p&gt;

&lt;p&gt;According to docmentation :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;As part of the library an small Matrix emulator is provided. 
The emulator renderize a virtual RGB matrix on a window in your desktop, without needing a real 
RGB matrix connected to your computer.

To execute the emulator set the MATRIX_EMULATOR environment variable to 1, then when NewRGBLedMatrix is used, 
a emulator.
Emulator is returned instead of a interface the real board.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To do that, type:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;export MATRIX_EMULATOR = 1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and now if you run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.go&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/xXcPR98N&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/FHxnhRQp/Screenshot-from-2021-08-21-12-44-45.png&quot; alt=&quot;Screenshot-from-2021-08-21-12-44-45.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally! After all this.&lt;/p&gt;

&lt;p&gt;This cloning method and following stuff my look bad to some. But please note. I did this because it gives better control to library. I can edit whatever i want or create anything new depending upon the use case.&lt;/p&gt;

&lt;p&gt;All done!&lt;/p&gt;

&lt;p&gt;Next time with something new :)&lt;/p&gt;

&lt;p&gt;Ciao&lt;/p&gt;

</description>
        <pubDate>Sun, 08 Aug 2021 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2021/08/08/golang-rgbmatrix/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2021/08/08/golang-rgbmatrix/</guid>
        
        <category>go</category>
        
        <category>ledmatrix</category>
        
        <category>tutorial</category>
        
        <category>raspberrypi</category>
        
        
      </item>
    
      <item>
        <title>[Book] Atomic Habits (Chapter One) : The Surpise Power of Atomic Habits</title>
        <description>&lt;h1 id=&quot;brief-&quot;&gt;Brief :&lt;/h1&gt;

&lt;p&gt;Author in the first chapter of the book discuss alot key points. He starts by sharing his life experience, near death life experience. Where he was critically injured and on the process of recovery. He found some way of creating habits through his experience as he was totally starting from scratch. It’s been gem of several pages to read. Ofcourse, the tips he share will not be provided with bulletin points straight away but more with context and what worked for author. And if we can learn from it anything.&lt;/p&gt;

&lt;p&gt;He describes the backbone of the book is his four step model of habits :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cue&lt;/li&gt;
  &lt;li&gt;Craving&lt;/li&gt;
  &lt;li&gt;Response&lt;/li&gt;
  &lt;li&gt;Reward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;People are interested in psychology may relate it with their academics.&lt;/p&gt;

&lt;h1 id=&quot;brits-cycling-effort-&quot;&gt;Brits Cycling Effort :&lt;/h1&gt;

&lt;p&gt;In a nutshell,&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“The Whole principle came from the idea that if you broke down everything you could think of that goes into doing something, and then improve it by 1 percent, you will get a significant increase when you put them all together”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It all starts with a small story where author talks about ‘British Cycling’. The Fate changed in 2003. When governing body for professional cycling in Great Britain had recently hired Dave Brailsford as it’s new performance director. At the time, professional cyclist in Great Britain were going through pretty rough and lean patch. Since 1908, British riders only own just a single gold medal at the Olympic Games. In 110 years, no british cylist had ever won the event.&lt;/p&gt;

&lt;p&gt;In fact, the performance was so down the slope and underwhelming. That one of the top bike manufactureres in Europe refused to sell bikes to the team because they were scared that will hurt the sales if other professionals saw the brits using this gear.&lt;/p&gt;

&lt;p&gt;But now things were going to be changed. Not due to luck. But the system they were going to employ in their learning and training. Dave Brailsford was hired to put up a new trajectory. What made him different was he was strongly committed to strategy that he referred as “the aggregation of marginal gains”. Which was the philosophy of searching of tiny margin of improvement in everything you do. So, Brailsford changed small things that from changing sleeping mattress to best way to wash hands. And hundred of these small improvement accumulated and result came faster.&lt;/p&gt;

&lt;p&gt;If you are thinking they improved over night with the changes. NO. You need to have patience and consistent effort which makes sure you are honest about what you do. After 5 Years of their new system. In 2008 olymics games in beijing. Where they won astounding 60% of gold medals available.&lt;/p&gt;

&lt;p&gt;IMPRESSIVE! Isn’t it?&lt;/p&gt;

&lt;p&gt;They created a system which allowed them to optimise their process. And that’s what matters i believe.&lt;/p&gt;

&lt;p&gt;Deep details given by &lt;a href=&quot;https://jamesclear.com/atomic-habits/cycling&quot;&gt;this blog&lt;/a&gt; about what did they do.&lt;/p&gt;

&lt;h1 id=&quot;why-small-habit-makes-difference-&quot;&gt;Why Small Habit makes difference ?&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is so easy to overestimate the importance of a one defining moment and underestimate the value of making small improvements on a daily basis. Too often, we convince ourselves that massive succes requires massive action.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I like this part where author beautifully explains how small improvement sound lackluster comparitive to big dreamy actions. He says “Whether it is losing weight, building a business, writing a book or achieving any goal, we put pressure on ourselves to make some earth shattering improvement everyone talks about.”&lt;/p&gt;

&lt;p&gt;Cannot agree more with it. While improvment of 1% looks so small that people think they don’t have time for it(i have been on that end). But the fact is, with small improvement over a period of time gives compounding effect. And that’s not opinion or inferences. It’s a fact.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/QVcM2g1x&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/44ShhB3t/betterbetter.png&quot; alt=&quot;betterbetter.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With one percent of gain you can see how your graph progress exponentially. And i bet you definately dont want to be on worse everyday end. What starts as a small win or a minor setback accumulates into something more. 
“Habits are compound interest of self improvement”. If you shift your airplane route by 1 degree the airplane will end up travelling to totally different location and outcome will be very visible. That’s why we should be keen and attentive toward small margins.&lt;/p&gt;

&lt;p&gt;Similarly, slight change in your everyday life can guide you to totally different path.&lt;/p&gt;

&lt;p&gt;Upcoming part is my all time favorite thing to read!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“It doesn’t matter if you are successful or not. What matters is whether your habits are putting you on the path toward success. You should be more concerned with your current trajectory than with your current result.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;and THAT IS THING IS GOLD! I just totally relate with it. We have lived enough time through the life to understand that good results are not always immediate. Then why don’t we learn from it.  Well, Its about the way we are going not our current results.&lt;/p&gt;

&lt;p&gt;Another favourite part of mine is :&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Your outcomes are a lagging measure of your habits. Your net worth is a lagging measure of your financial habits. Your weight is a lagging measure of your eating habits. Your knowledge is a lagging measure of your learning habits. Your clutter is a lagging measure of your cleaning habits. You get what you repeat.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At the end of the day, what we face is the residue of our thoughts, work etc! (Most of the times though).&lt;/p&gt;

&lt;h1 id=&quot;mastery-requires-patience-&quot;&gt;Mastery requires patience :&lt;/h1&gt;

&lt;p&gt;Imagine, you have a ice cube. You keep it on room in an open space. And rooms slowly heats up, 
26 C -&amp;gt; No melting
27 C -&amp;gt; No melting
28 C -&amp;gt; No melting
29 C -&amp;gt; No melting
30 C -&amp;gt; No melting
31 C -&amp;gt; No melting (still nothing happened)
Now at,
32 C -&amp;gt; starts melting&lt;/p&gt;

&lt;p&gt;now it just unlocked huge change. So, breakthrough moments are often result of many previous actions, which builds up the potential required to unleash a major change.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cancer spends 80% of its life undetectable, then takes over the body in a month&lt;/li&gt;
  &lt;li&gt;Bamboo barely grows in first five years as it builds extensive root system within it and BOOM after that it grows 90ft. within 5 weeks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s about the system we use for our life. Results are going to be by products. 
Overnight success is a myth. Author points out the phase where you really doing everything right as a process but still not seeing results yet as ‘Plateau of Latente potential’. Where you work your heart out but need to persist long enough to break through this plateau.&lt;/p&gt;

&lt;p&gt;Habits often appear to make no difference until you cross that critical threshold and unlock a new level of performance. PATIENCE and GRIT are basically two fundamental in these condition.&lt;/p&gt;

&lt;p&gt;There is a quote from Jacob Riss : “When nothing seem to help, i go and look at a stonecutter hammering away at his rock, perhaps a hundred times without as much as a crack showing in it. Yet, at the hundred and first blow will split in two, and i know it was not that last blow that did it – but all that had gone before.”&lt;/p&gt;

&lt;p&gt;First chapter just like classic example of “To build a extraordinary building, you need to break old ones down and rebuild again”. In first chapter, there are lot of demystification going on. So that in later chapters it’s more easy to build lot of things from the belief we have learned.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;

</description>
        <pubDate>Mon, 15 Mar 2021 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2021/03/15/atomic-habits-ch01/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2021/03/15/atomic-habits-ch01/</guid>
        
        <category>atomichabits</category>
        
        <category>habits</category>
        
        <category>book</category>
        
        <category>learning</category>
        
        <category>system</category>
        
        
      </item>
    
      <item>
        <title>[Random] An Era of Ignorance</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Disclaimer: Before you proceed, make sure you take this post as my opinion which do not intend to attack anyone. Proceed only when you want to learn about others perception and how they see the world and if you are open to expansion to thoughts and learnings. You are free to discuss your thoughts in comment section.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;brief-&quot;&gt;Brief :&lt;/h1&gt;
&lt;p&gt;As title suggests, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;An Era of Ignorance&lt;/code&gt;. I believe, we live in the era of Ignorance. Where we believe something..do something And both contradicts. Totally lacking the clarity about world. For example: Everyone wants of buy a ferarri, BMW, Jaguar etc. And at the sametime, they want to conserve nature too. Do things work like that? I don’t believe so. I will be sharing some of my understanding on the topic.&lt;/p&gt;

&lt;p&gt;Well, among all the chaos going on. We sure do not have proper description about clarity. We even may remember number of times when we were clear about something. Because, it’s so minimal and countable that probably easy to remember. By clarity i mean, pin pointness toward the target. When you are so clear about what you believe in that everything becomes breeze only your belief stand firm like mountain.&lt;/p&gt;

&lt;p&gt;People (or maybe including me) really really want to save climate –but do not want to sacrifice enough what it takes in their life to do so.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When was the last time, you achieved something without effort?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;scratching-the-surface-&quot;&gt;Scratching the surface :&lt;/h1&gt;

&lt;p&gt;Comfort level has killed our ability to make a difference. I often hear people say, “I want to earn lots of money”. When money becomes utmost purpose of life. Then there is something wrong. Money is important like anything else in life. My only point is, when it is whole purpose of life. You will try earning it by any means possible and in some cases some ‘means’ could lead to catastrophic results.&lt;/p&gt;

&lt;p&gt;When your whole purpose of life is to make a difference then ‘earning money’ automatically becomes part of your endeavour. There are layers that you would need to unveil.&lt;/p&gt;

&lt;p&gt;When we hear the word ethics. It will depend what ‘ethics’ you subscribe to. So, it’s understandable that defination and form of ethics may vary from people to people. ~ BUT Is it so? Is there no real form of ethics? Is it just subjective to people? Definitely, there is a difference between stepping on the foot of a begger and giving them food. There is a difference. Indeed. There is only one form of ethics which is real. So real that it gives you peace.&lt;/p&gt;

&lt;p&gt;Imagine, you are standing infront of 2 mirrors. Facing each other. If you look into the mirrors. You will see thousands of ‘you’. It doesn’t mean there are thousands of you in real. You are only one and real. Similarily, same with ethics. Real ethics have gone scattered amidst the fake ethics which people propose for their own comfort. Which is illusionary itself. Real ethics remains pretty much universal. Which are basis of life and peace. Morals which helps society. Which are so pure yet solid. Ethics which is devoid of hatred, disdain, discouragement, ego, etc. Which is down to earth. Hopefully, you will understand ethics are not set of rules. That can’t be memorized. They are way of life. You will learn them as you contemplate on the life situations.&lt;/p&gt;

&lt;h1 id=&quot;is-perfection-really-ideal-state-or-practical&quot;&gt;Is perfection really ideal state? Or practical?&lt;/h1&gt;
&lt;p&gt;I have heard people say,&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“If we cannot be perfect. Why to even seek it?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We cannot be perfect. But we can be reasonably good. The idea of perfection helps us to push our limits. Doesn’t allow to be complacent. It’s same like you cannot be perfect at maths but does it mean it should be entertained with no effort of improvement?&lt;/p&gt;

&lt;p&gt;Just because, you perfectly cannot help anyone doesn’t mean you shouldn’t.&lt;/p&gt;

&lt;p&gt;Imagine life as a bag of balls where you have bad and good balls filled within it. (Nothing wrong with it). So, the way you will sort out bad balls out of good ones is by hit and trail. In real life, it’s theory of learning from experience. You do something. You learn from it. Then you do it better. As simple as that. Not easy though. Sadly, people misintrepet the word &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;simple&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;easy&lt;/code&gt;. Yes, things can be simple and difficult. Going from point A to point B could be simple. Nothing complicated with that but it doesn’t have to be easy. So, something can be simple and yet difficult. And complex and yet easy.&lt;/p&gt;

&lt;p&gt;Similarly, to be less ignorant and more concious in our life. It’s practice. Practice of contemplation. Thinking everytime from every possible dimension. And most importantly, always looking to improve.&lt;/p&gt;

&lt;h1 id=&quot;how-perception-is-the-key-&quot;&gt;How perception is the key ?&lt;/h1&gt;

&lt;p&gt;You can’t control what’s coming to you. But you can &lt;em&gt;control&lt;/em&gt; how you handle it. Someone can make fun of east asian about squinting eyes. But when someone really takes in good spirit then even people who do it with the purpose of disdain they lose. Not that i say people who make this racist mark are not problem. But, those people are always going to be there. We can’t do anything about. But, if we understand that they are doing due to some limited capabilites then it becomes funny, childish.&lt;/p&gt;

&lt;p&gt;Most of the situation in life would be defined by how we take a particular situation as. Either, we could unleash the wrath. Or, we can be calm. Every situation demands different from us. In some, we need to be in wrath. Calmness would only do damage. And in some, calmness is silver bullet. Everything is situation dependent.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Asking ourselves, 
What this situation demand us from?
What would be the best thing to do? 
Can i see more deep in the situation intricacies?&lt;br /&gt;
…. and so on..&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There is a saying that, Anger burns the person itself. Mostly in life, the situation we would encounter, most of them need no involvement of anger whatsoever. When someone being mean to you and you feeling angry, tell yourself that other person is still in learning process. Or, Maybe person is having a bad day. 
Anything that may help you to feel compassion toward that being. Perception is not an angle. It’s a state where you able to see everything in clarity. Being said that, perception it’s attained throughout life experiences. As we all are in some kind of learning process.&lt;/p&gt;

</description>
        <pubDate>Thu, 04 Mar 2021 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2021/03/04/An_Era_of_Ignorance/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2021/03/04/An_Era_of_Ignorance/</guid>
        
        <category>random</category>
        
        <category>spirituality</category>
        
        <category>life</category>
        
        <category>perception</category>
        
        
      </item>
    
      <item>
        <title>Intuition behind Random Forest</title>
        <description>&lt;h1 id=&quot;what-are-we-gonna-do-in-this-blog&quot;&gt;What are we gonna do in this blog?&lt;/h1&gt;
&lt;p&gt;Learning about what goes behind random forest. What is it that beginner struggles understanding random forest. What makes random forest so powerful. Further, we will discuss intuition behind random forest. Explaining random forest in detail with own handcrafted code and sklearn provided.&lt;/p&gt;

&lt;h1 id=&quot;personal-experience&quot;&gt;Personal Experience&lt;/h1&gt;
&lt;p&gt;When i was starting out with Machine Learning. All i could see the hype of Random Forest being powerful and quite handy in use. I was abit curious. So, i started learning about it in depth. It became a bit hard to comprehend the words like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ensemble&lt;/code&gt; and random forest in general. But after understanding it, i can say it’s really makes sense why random forest works. I’ll be sharing what are my key insights on this.&lt;/p&gt;

&lt;h1 id=&quot;what-is-the-intuition-behind-random-forest&quot;&gt;What is the intuition behind Random Forest?&lt;/h1&gt;
&lt;p&gt;Random Forest is derived from the concept of Decision Tree. Assuming you know what decision trees are. In a nutshell, When we use lots of decision trees (i.e. 20,100,150) to train &amp;amp; predict the input data. This lot (or Group) of decision trees are known as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ensemble&lt;/code&gt;. Rather than using single decision tree to finalize decision. We use many of them. It’s same as imagine you have to decide which book to buy for statistics. When you do is talking to many people and then buy which most of the people prefer.&lt;/p&gt;

&lt;p&gt;Another example: Let’s say you going to interview. So there will be different rounds of interview and you will be evaluated on different features of yours. Finally, getting selected or not would be what most of the interviewer perceive. If 2 out of 3 agree with selection you will be selected. If 2 out of 3 don’t agree with selection, then you will be rejected.&lt;/p&gt;

&lt;p&gt;Same Goes in Random Forest: You take many decision trees (it could be thousands too), for now let’s say 100 and each of them trained on random data points(row) of training data. Then you use, testing data and each decision tree will predict according to the feature they have been trained on.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Important: All the decision trees shouldn’t be trained on same features. That could lead to biased predictions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Training on random features(reason why random forest are called random) allows to create more flexible and ungreedy algorithm. Decision tree uses &lt;a href=&quot;https://www.edureka.co/community/46109/what-is-greedy-approach-in-decision-tree-algorithm&quot;&gt;greedy algorithm&lt;/a&gt;.
So, it’s important to feed random features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I hope, you get a bit clarity what it means when we say random forest. Although, your concepts will be totally cleared when you dive into coding the decision tree from scratch.&lt;/p&gt;

&lt;h1 id=&quot;coding-our-own-random-forest&quot;&gt;Coding our own Random forest&lt;/h1&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt; from sklearn.datasets import make_moons
&amp;gt; from sklearn.model_selection import train_test_split

&amp;gt; mo= make_moons(n_samples=10000, noise=0.4) #creating toy data
&amp;gt; X_train, X_test, y_train, y_test= train_test_split(mo[0], mo[1], test_size=0.2, random_state=42)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Small peak into data and label.We are looking at first 5 data points and their label(or target). As you can see below, Left side array is X and Y value. And Right Side Array is labels of respective row value.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt; X_train[:5], y_train[:5]

(array([[ 0.10186633, -0.20643133],
        [-0.24668162,  1.0486827 ],
        [-0.57215016,  0.30076258],
        [ 0.05560597,  0.9361636 ],
        [-0.91425428, -0.33931685]]), array([1, 0, 0, 1, 0]))
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Just a simple look at our data. Green and Purple are the different classes and in this we have to predict if X and Y coordinates are provided what will be the class of the point.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;%matplotlib inline
import matplotlib.pyplot as plt

plt.scatter(mo[0][:,0], mo[0][:,1], c=mo[1], s=1)
plt.show()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/hedwig_explains/img/moon.png&quot; alt=&quot;plot&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 11 Feb 2021 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2021/02/11/intuition-behind-random-forest/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2021/02/11/intuition-behind-random-forest/</guid>
        
        <category>randomforest</category>
        
        <category>datascience</category>
        
        <category>intuition</category>
        
        
      </item>
    
      <item>
        <title>Implementation of MobileNet Model</title>
        <description>&lt;p&gt;&lt;img src=&quot;data:image/png;base64,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&quot; alt=&quot;xception&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-allows-us-to-understand-paper&quot;&gt;Abstract allows us to understand paper:&lt;/h1&gt;

&lt;p&gt;We present a class of efficient models called MobileNets
for mobile and embedded vision applications. MobileNets
are based on a streamlined architecture that uses depth-
wise separable convolutions to build light weight deep
neural networks. We introduce two simple global hyper-
parameters that efficiently trade off between latency and
accuracy. These hyper-parameters allow the model builder
to choose the right sized model for their application based
on the constraints of the problem.&lt;/p&gt;

&lt;p&gt;We present extensive
experiments on resource and accuracy tradeoffs and show
strong performance compared to other popular models on
ImageNet classification. We then demonstrate the effective-
ness of MobileNets across a wide range of applications and
use cases including object detection, finegrain classification, face attributes and large scale geo-localization.&lt;/p&gt;

&lt;h1 id=&quot;in-a-nutshell&quot;&gt;In a nutshell:&lt;/h1&gt;
&lt;p&gt;The mobilenet paper is inspired from Xception paper (&lt;a href=&quot;https://hiteshhedwig.github.io/hedwig_explains/2020/10/28/intutition-behind-xception-model/&quot;&gt;my implementation&lt;/a&gt;). While in xception network. Author used depthwise convolutions followed by pointwise convolutions. This helped decreasing computational cost of the entire model quite much. Entire aim of mobilenet paper was to deliver state of the art model whose size is significantly less than previous SOTA(state-of-the-art) models like: VGG19, ResNet(34,50 etc), Inception etc.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/0KMM7NBk&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/x8gvDJGz/mb1.png&quot; alt=&quot;mb1.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you can see above, MobileNet model got 4.2 million parameters. Comparison to GoogleNet (6.8 million), VGG16 (128 million). That’s what is helpful. And suggested by this paper.  MobileNet is nearly as accurate as VGG16 while being 32 times smaller and  27 times less compute intensive. It is more accurate than
GoogleNet while being smaller and more than 2.5 times less
computation.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: MobileNet comparison is not only limited to GoogleNet or VGG but also applicable to PlaNet, CoCo object detection etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/zL2GprWV&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/FRNkrFHg/mb2.png&quot; alt=&quot;mb2.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;mobilenet-main-discussion&quot;&gt;MobileNet Main Discussion:&lt;/h1&gt;

&lt;p&gt;For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1 × 1 convolution to combine the
outputs the depthwise convolution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/fkmMSXH3&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/SQtR5fCG/mb3.png&quot; alt=&quot;mb3.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you can see above. Instead of Standard Convolution strategy, we are using 3x3 depthwise convolution followed by Batch Normalization and ReLU. Later 1x1 pointwise convolution to complete our depthwise separable convolution layer.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Depthwise separable convolution layer :
    &lt;ul&gt;
      &lt;li&gt;3x3 depthwise convolution&lt;/li&gt;
      &lt;li&gt;1x1 pointwise convolution&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This factorization has the effect of
drastically reducing computation and model size. Depthwise separable convolution are made up of two
layers: depthwise convolutions and pointwise convolutions.
We use depthwise convolutions to apply a single filter per
each input channel (input depth). Pointwise convolution, a
simple 1×1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets
use both batchnorm and ReLU nonlinearities for both layers.&lt;/p&gt;

&lt;h1 id=&quot;architecture&quot;&gt;Architecture:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Conv dw -&amp;gt; Depthwise convolution&lt;/li&gt;
  &lt;li&gt;s -&amp;gt;  stride&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers. All layers are followed by a batchnorm and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/rd1vFCyc&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/cJ2ZhXVK/mb4.png&quot; alt=&quot;mb4.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Important: As you can see above image got one ‘typo’ word. I was reading the paper. And i think, stride should be 1 (s1) not s2. So incase you are implementing paper on your own. Make sure you don’t miss this.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;code&quot;&gt;Code&lt;/h1&gt;

&lt;p&gt;Implementing depthwise separable convolution.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import torch
import torch.nn as nn

class depthwise_pointwise(nn.Module):
    def __init__(self, channels, stride=1):
        super(depthwise_pointwise, self).__init__()
        self.depthwise_layer= nn.Sequential(
            nn.Conv2d(channels[0], channels[0],3,
                      groups=channels[0], padding=1, stride=stride),
            nn.BatchNorm2d(channels[0]),
            nn.ReLU(),
            nn.Conv2d(channels[0],channels[1],1),
            nn.BatchNorm2d(channels[1]),
            nn.ReLU(),
        )
    
    def forward(self,x):
        return self.depthwise_layer(x)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Main Mobilenet structure:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;class Mobilenet(nn.Module):
    def __init__(self,depthwise_pointwise):
        super(Mobilenet, self).__init__()
        self.conv1= nn.Conv2d(3,32, 3, stride=2, padding=1)
        self.bn1= nn.BatchNorm2d(32)
        self.relu= nn.ReLU()
        #depthwise sep
        self.dw_s1= depthwise_pointwise([32,64], stride=1)
        self.dw_s2= depthwise_pointwise([64,128], stride=2)
        self.dw_s3= depthwise_pointwise([128,128])
        self.dw_s4= depthwise_pointwise([128,256], stride=2)
        self.dw_s5= depthwise_pointwise([256,256])
        self.dw_s6= depthwise_pointwise([256,512], stride=2)
        # x5 layer stack
        self.dw_x5= depthwise_pointwise([512,512])
        # dw layer
        self.dw_s7= depthwise_pointwise([512,1024], stride=2)
        self.dw_s8= depthwise_pointwise([1024,1024])
        #avg pool
        self.avgpool= nn.AdaptiveAvgPool2d((1,1))
        self.fc= nn.Linear(1024,1000)


    def forward(self,x):
        x= self.relu(self.bn1(self.conv1(x)))
        print(x.size())
        x= self.dw_s1(x)
        print(x.size())
        x= self.dw_s2(x)
        print(x.size())
        x= self.dw_s3(x)
        print(x.size())
        x= self.dw_s4(x)
        print(x.size())
        x= self.dw_s5(x)
        print(x.size())
        x= self.dw_s6(x)
        print(x.size())
        for i in range(5):
            x= self.dw_x5(x)
        print(x.size())
        x= self.dw_s7(x)
        print(x.size())
        x= self.dw_s8(x)
        print(x.size())
        x= self.avgpool(x)
        print(x.size())
        x=x.view(x.shape[0],-1)
        x= self.fc(x)
        print(x.size())

        return x
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;mb= Mobilenet(depthwise_pointwise)
img= torch.randn(1,3,224,224)
k=mb(img)

&amp;gt; Output:
torch.Size([1, 32, 112, 112])
torch.Size([1, 64, 112, 112])
torch.Size([1, 128, 56, 56])
torch.Size([1, 128, 56, 56])
torch.Size([1, 256, 28, 28])
torch.Size([1, 256, 28, 28])
torch.Size([1, 512, 14, 14])
torch.Size([1, 512, 14, 14])
torch.Size([1, 1024, 7, 7])
torch.Size([1, 1024, 7, 7])
torch.Size([1, 1024, 1, 1])
torch.Size([1, 1000])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;mobilenet-architecture-&quot;&gt;MobileNet Architecture :&lt;/h1&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Mobilenet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (dw_s1): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (dw_s2): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (dw_s3): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (dw_s4): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (dw_s5): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (dw_s6): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (dw_x5): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (dw_s7): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (dw_s8): depthwise_pointwise(
    (depthwise_layer): Sequential(
      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=1024, out_features=1000, bias=True)
)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sun, 01 Nov 2020 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2020/11/01/intuition-behind-mobilenet-model/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2020/11/01/intuition-behind-mobilenet-model/</guid>
        
        <category>code</category>
        
        <category>researchpaper</category>
        
        <category>intuition</category>
        
        <category>deeplearning</category>
        
        <category>computervision</category>
        
        
      </item>
    
      <item>
        <title>Implementation of Xception Model</title>
        <description>&lt;p&gt;&lt;img src=&quot;https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR6duP6j9BjgUWQ26-l22ONwGXfyOvelAfUKQ&amp;amp;usqp=CAU&quot; alt=&quot;xception&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-are-we-doing-in-this-blog&quot;&gt;What are we doing in this blog?&lt;/h1&gt;
&lt;p&gt;If you are here, then you know what are we doing. But still for the sake of clarity:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We are implementing popular Xception Network as represented by this &lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;paper&lt;/a&gt;. In this blog, i’ll be sharing my notes and learnings from this paper &amp;amp; Implemented code as well.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;intro&quot;&gt;Intro:&lt;/h1&gt;
&lt;p&gt;After GoogLeNet proposal of inception module have led in Surge of various improved inception network. In case, you are instantly thinking about the movie ‘inception’. Yeah, that’s right. It’s taken from there. Convolutions within convolutions, this is inception module.&lt;/p&gt;

&lt;p&gt;But what does inception module does differently?&lt;/p&gt;

&lt;p&gt;It significantly reduces computational costs. Idea is not only about making accurate models but also making them deployable in outer world hardwares. Andrew Ng explains it the best in this &lt;a href=&quot;https://www.youtube.com/watch?v=C86ZXvgpejM&quot;&gt;video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below, Image represents standard GoogLeNet proposed inception module. 
1x1 for dimensions reductions. And later on we are concatenating the output of the each branch of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/TLxZ8fh2&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/cHtZnJS3/xcep1.png&quot; alt=&quot;xcep1.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;Paper&lt;/a&gt; - “Rethinking the Inception Architecture for Computer Vision” talks about getting rid of 5x5 convolutions and substituting it with two 3x3 convolution layers. This can result in dramatic decrease in computational costs from 120 millions to 1.2 million parameters &amp;amp; this is a giant leap of faith.
Author also suggests using asymmetric convolutions, e.g. n × 1. For example: using a 3 × 1 convolution followed by a 1 × 3 convolution
is equivalent to sliding a two layer network with the same receptive field as in a 3 × 3 convolution. But in practicality following issue was addressed in paper: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;we have found that employing this factorization does not work well on early layers, but it gives very good results on medium grid-sizes (On m × m feature maps, where
m ranges between 12 and 20).&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/crPky0bq&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/9QQ69fPf/xcep2.png&quot; alt=&quot;xcep2.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After that author of Xception &lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;paper&lt;/a&gt; brought new improvement in the module which gives more better compactness and fairly easy implementation in code.&lt;/p&gt;

&lt;p&gt;But what is this xception?&lt;/p&gt;

&lt;p&gt;Slightly different Xception is! Ofcourse, its an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extreme Inception module&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;xception-complexity-yet-simplicity&quot;&gt;Xception complexity, yet simplicity:&lt;/h1&gt;

&lt;p&gt;Now after you have fair idea of what inception module looks like how it improved.&lt;/p&gt;

&lt;p&gt;An “extreme” version of an Inception module, based on
this stronger hypothesis, would first use a 1x1 convolution to
map cross-channel correlations, and would then separately
map the spatial correlations of every output channel.&lt;/p&gt;

&lt;p&gt;This might be easily explained with the help of image:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/rKRg97Ff&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/jdZ9RbVY/xcep3.png&quot; alt=&quot;xcep3.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you can see, Input (let’s say 64x150x150) goes through 1x1 convolutions and the output of this uses various 3x3 convolutions. This seperately goes through every output channels. This is almost same as depthwise seperable convolutions. Commonly called
“separable convolution”. This is, a spatial convolution performed independently over each channel of an input, followed by a pointwise convolution, i.e. a 1x1 convolution, projecting the channels output by the depthwise convolution onto a new channel space.&lt;/p&gt;

&lt;p&gt;Incase, this looks hard to under no worries we will understand it in code!
In tensorflow and keras it’s pretty easy to deal with separable conv. But in pytorch we have to change only one parameter in normal &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Conv2d&lt;/code&gt;. As follows: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Conv2d(in_channel, out_channel,3, groups=in_channel, padding=1)&lt;/code&gt;. ‘groups’ argument must be multiple of in_channel. This followed by  pointwise convolution will complete our aim.&lt;/p&gt;

&lt;p&gt;In pytorch official documentation, (At the time of writing this blog) they don’t say much about depthwise seperable conv.&lt;/p&gt;

&lt;h2 id=&quot;depthwise-separable-convolutions&quot;&gt;DepthWise Separable Convolutions&lt;/h2&gt;

&lt;p&gt;Depthwise separable convolutions as usually implemented perform first channel-wise spatial convolution and then perform 1x1 convolution, whereas Inception performs the 1x1 convolution first. Below is a sample of how seperable Conv. is implemented –&lt;/p&gt;

&lt;p&gt;Channel-wise spatial 3x3 convolution:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Conv2d(channels[0], channels[0],3, groups=channels[0], padding=1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Point-wise 1x1 convolution:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Conv2d(channels[0], channels[1],1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;These will be used often in the Xception architecture. Before we proceed let’s look at complete architecture so we know what are we doing..&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/sBmJXrwg&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/TwMNt2bg/xcep4.png&quot; alt=&quot;xcep4.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Dividing entire network into 3 sections:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Entry Flow&lt;/li&gt;
  &lt;li&gt;Middle Flow (repeated 8 times)&lt;/li&gt;
  &lt;li&gt;Exit Flow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you look closely, you will see skip connections also (i.e the Conv 1x1 layer which take previous input layer and adding in later layer). This is a bit similar with ResNet? Using residual layers and adding them up! This seen to significantly improve the accuracy of model without overfitting.&lt;/p&gt;

&lt;p&gt;We are first implementing DepthWiseSeperable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: Class DepthWiseSeperable include whole block between the skip connection as you can see in above image.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In class below we are taking channels list input. Which further will be used for parameters of seperable convolutions.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;class DepthWiseSeparable(nn.Module):
    def __init__(self, channels, middle_flow= False):
        super(DepthWiseSeparable, self).__init__()
        
        # block for entry flow
        if middle_flow== False: 
        
            self.block= nn.Sequential(
                nn.ReLU(),
                nn.Conv2d(channels[0], channels[0],3, groups=channels[0], padding=1),
                nn.Conv2d(channels[0], channels[1],1),
                nn.BatchNorm2d( channels[1]),
                nn.ReLU(),
                nn.Conv2d( channels[1], channels[1],3, groups=channels[1], padding=1),
                nn.Conv2d( channels[1], channels[1],1),
                nn.BatchNorm2d(channels[1]),
                nn.MaxPool2d(3,stride=2, padding=1),
            )
        
        # block for middle flow
        if middle_flow==True:
            self.block= nn.Sequential(
                nn.ReLU(),
                nn.Conv2d(channels[0], channels[0],3, groups=channels[0], padding=1),
                nn.Conv2d(channels[0], channels[1],1),
                nn.BatchNorm2d( channels[1]),
                nn.ReLU(),
                nn.Conv2d( channels[1], channels[1],3, groups=channels[1], padding=1),
                nn.Conv2d( channels[1], channels[1],1),
                nn.BatchNorm2d(channels[1]),
                nn.ReLU(),
                nn.Conv2d( channels[1], channels[1],3, groups=channels[1], padding=1),
                nn.Conv2d( channels[1], channels[1],1),                
            )

        # block for exit flow
        
        if middle_flow=='Exit':
            self.block= nn.Sequential(
                nn.Conv2d(channels[0], channels[0],3, groups=channels[0], padding=1),
                nn.Conv2d(channels[0], channels[1],1),
                nn.BatchNorm2d( channels[1]),
                nn.ReLU(),
                nn.Conv2d( channels[1], channels[1],3, groups=channels[1], padding=1),
                nn.Conv2d( channels[1], channels[2],1),
                nn.BatchNorm2d(channels[2]),
                nn.ReLU(),
            )


    
    def forward(self,x):
        x= self.block(x)
        return x

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is Main xception class which will use above class to create whole network.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;class Xception(nn.Module):
    def __init__(self, DepthWiseSeparable):
        super(Xception, self).__init__()

        self.conv1_3x3 = nn.Conv2d(3,32,3,stride=2,padding=1)
        self.bn1 =nn.BatchNorm2d(32)
        self.relu = nn.ReLU()

        self.conv2_3x3= nn.Conv2d(32,64,3,stride=1,padding=1)
        self.bn2 =nn.BatchNorm2d(64)
        self.relu= nn.ReLU()
        #Entry flow
        self.block_3x3= DepthWiseSeparable(channels=[64,128])
        self.downsample_1= nn.Conv2d(64,128,1,stride=2)

        self.block_3x3_256= DepthWiseSeparable(channels=[128,256])
        self.downsample_2= nn.Conv2d(128,256,1,stride=2)

        self.block_3x3_728= DepthWiseSeparable(channels=[256,728])
        self.downsample_3= nn.Conv2d(256,728,1,stride=2)
        #Middle Flow
        self.block_3x3_middle= DepthWiseSeparable(channels=[728,728], middle_flow=True)
        #Exit flow
        self.block_3x3_exit = DepthWiseSeparable(channels=[728,1024])
        self.downsample_4= nn.Conv2d(728,1024,1,stride=2)

        self.block_3x3_exit_2 = DepthWiseSeparable(channels=[1024,1536,2048], middle_flow='Exit')
        
        self.avgpool= nn.AdaptiveAvgPool2d((1,1))

        self.fc= nn.Linear(2048,1000)

    def forward(self,x):
        x= self.relu(self.bn1(self.conv1_3x3(x)))
        print(x.size())
        x_identity = self.relu(self.bn2(self.conv2_3x3(x)))
        print(x_identity.size())
        x_res_1= self.block_3x3(x_identity)
        print(x_res_1.size())
        x_identity= self.downsample_1(x_identity)
        print(x_identity.size())
        
        x_identity= x_identity+x_res_1

        x_res_2= self.block_3x3_256(x_identity)
        print(x_res_2.size())
        x_identity= self.downsample_2(x_identity)
        print(x_identity.size())

        x_identity= x_identity+x_res_2
    
        x_res_3= self.block_3x3_728(x_identity)
        print(x_res_3.size())
        x_identity= self.downsample_3(x_identity)
        print(x_identity.size())
        x_identity= x_identity+x_res_3

        for i in range(9):
            x_res_4= self.block_3x3_middle(x_identity)
            x_identity= x_identity+x_res_4

        print(x_identity.size())
        
        x_res_5= self.block_3x3_exit(x_identity)
        x_identity= self.downsample_4(x_identity)

        x_identity= x_identity+x_res_5
        print(x_identity.size())

        x_res_6= self.block_3x3_exit_2(x_identity)
        print(x_res_6.size())

        x= self.avgpool(x_res_6)
        print(x.size())

        x= x.view(x.shape[0],-1)
        print(x.size())

        x= self.fc(x)
        print(x.size())       
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;img= torch.randn(1,3,299,299)

net= Xception(DepthWiseSeparable)
net(img)

&amp;gt; Output:
torch.Size([1, 32, 150, 150])
torch.Size([1, 64, 150, 150])
torch.Size([1, 128, 75, 75])
torch.Size([1, 128, 75, 75])
torch.Size([1, 256, 38, 38])
torch.Size([1, 256, 38, 38])
torch.Size([1, 728, 19, 19])
torch.Size([1, 728, 19, 19])
torch.Size([1, 728, 19, 19])
torch.Size([1, 1024, 10, 10])
torch.Size([1, 2048, 10, 10])
torch.Size([1, 2048, 1, 1])
torch.Size([1, 2048])
torch.Size([1, 1000])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;model-&quot;&gt;Model :&lt;/h2&gt;
&lt;p&gt;Our model will look like something like&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Xception(
  (conv1_3x3): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (conv2_3x3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (block_3x3): DepthWiseSeparable(
    (block): Sequential(
      (0): ReLU()
      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
      (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): ReLU()
      (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
      (6): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
  )
  (downsample_1): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))
  (block_3x3_256): DepthWiseSeparable(
    (block): Sequential(
      (0): ReLU()
      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
      (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): ReLU()
      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
  )
  (downsample_2): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))
  (block_3x3_728): DepthWiseSeparable(
    (block): Sequential(
      (0): ReLU()
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
      (2): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1))
      (3): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): ReLU()
      (5): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728)
      (6): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
  )
  (downsample_3): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2))
  (block_3x3_middle): DepthWiseSeparable(
    (block): Sequential(
      (0): ReLU()
      (1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728)
      (2): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1))
      (3): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): ReLU()
      (5): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728)
      (6): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU()
      (9): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728)
      (10): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (block_3x3_exit): DepthWiseSeparable(
    (block): Sequential(
      (0): ReLU()
      (1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728)
      (2): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1))
      (3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): ReLU()
      (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
      (6): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
  )
  (downsample_4): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2))
  (block_3x3_exit_2): DepthWiseSeparable(
    (block): Sequential(
      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
      (1): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1))
      (2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU()
      (4): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
      (5): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1))
      (6): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Wed, 28 Oct 2020 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2020/10/28/intutition-behind-xception-model/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2020/10/28/intutition-behind-xception-model/</guid>
        
        <category>code</category>
        
        <category>researchpaper</category>
        
        <category>intuition</category>
        
        
      </item>
    
      <item>
        <title>Changing Folder Name for model training</title>
        <description>&lt;h1 id=&quot;what-are-we-doing-in-this-blog&quot;&gt;What are we doing in this blog?&lt;/h1&gt;

&lt;p&gt;We are changing folders name like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Folder Name from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n02113023-Pembroke&lt;/code&gt; Changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pembroke&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Folder Name from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n02113624-toy_poodle&lt;/code&gt; Changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Toy Poodle&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Folder Name from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n02115641-dingo&lt;/code&gt; Changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dingo&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If want to learn this, Go on with reading:&lt;/p&gt;

&lt;p&gt;In this, dataset directory. I have downloaded, famous standford &lt;a href=&quot;http://vision.stanford.edu/aditya86/ImageNetDogs/&quot;&gt;dataset&lt;/a&gt;. The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization.&lt;/p&gt;

&lt;p&gt;The name of the folders (which will be used as classes while training) have names like, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n02093256-Staffordshire_bullterrier&lt;/code&gt;. We definitely don’t want our class to be like that but rather simple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Staffordshire Bullterrier&lt;/code&gt;. The python script will get rid of any weird characters that are unneccessary.&lt;/p&gt;

&lt;p&gt;Now, we have our path defined where the data is situated. In my case, i had uploaded it to google drive. Time consuming upload, avoid it if possible.&lt;/p&gt;

&lt;p&gt;Let’s say we have folder name like this:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n02093256-Staffordshire_bullterrier&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here is our process of obtaining what we want,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We first split on the basis of  - (hyphen) charachter&lt;/li&gt;
  &lt;li&gt;Then we obtain list like this: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;['n02093256' , 'Staffordshire_bullterrier']&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;After that, we use slicing technique to select second index.&lt;/li&gt;
  &lt;li&gt;Then after we have what we wanted, we can simply again split on the basis of  _ (underscore)&lt;/li&gt;
  &lt;li&gt;Then we obtain something like this:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;['Staffordshire','bullterrier']&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;After we have what we wanted we can simply join the list back. And use capitalize to make first letter of word capitalize.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;📝 Note: This code will need to be modified according to the folder name you are dealing with.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;'''
THIS PYTHON CODE CHANGES NAME OF FILES IN A FOLDER
'''

import os
import string


def change_name(folder_name): 
    folder_name=folder_name.split('-') #n02113023-Pembroke-&amp;gt;['n02113023','Pembroke'] 
    folder_name= ' '.join(folder_name[1:]) #Selecting index after 1 and joining them
    folder_name=folder_name.split('_') # Again splitting becayse name seem to be like Staffordshire_bullterrier
    folder_name= ' '.join(folder_name) #again joining after getting rid of _ 
    return string.capwords(folder_name) #or we can use capitalize()

#iterating over all the folders and changing file name
for fn in os.listdir(path):

    new_path= os.path.join(path,fn)
    #folder_name= os.path.basename(new_path) #alternative use of to know file name    
    new_folder_name= change_name(fn)
    os.rename(os.path.join(path,fn),os.path.join(path,new_folder_name))
    print (f'Folder Name from {fn} Changed to {new_folder_name}')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Output :
Folder Name from n02113023-Pembroke Changed to Pembroke
Folder Name from n02093256-Staffordshire_bullterrier Changed to Staffordshire Bullterrier
Folder Name from n02093428-American_Staffordshire_terrier Changed to American Staffordshire Terrier
Folder Name from n02113624-toy_poodle Changed to Toy Poodle
Folder Name from n02100236-German_short-haired_pointer Changed to German Short Haired Pointer
Folder Name from n02115641-dingo Changed to Dingo
Folder Name from n02089867-Walker_hound Changed to Walker Hound
Folder Name from n02099601-golden_retriever Changed to Golden Retriever
Folder Name from n02105162-malinois Changed to Malinois
Folder Name from n02100735-English_setter Changed to English Setter
Folder Name from n02097298-Scotch_terrier Changed to Scotch Terrier
Folder Name from n02095889-Sealyham_terrier Changed to Sealyham Terrier
Folder Name from n02106550-Rottweiler Changed to Rottweiler
Folder Name from n02088094-Afghan_hound Changed to Afghan Hound
Folder Name from n02112018-Pomeranian Changed to Pomeranian
Folder Name from n02099429-curly-coated_retriever Changed to Curly Coated Retriever
Folder Name from n02095314-wire-haired_fox_terrier Changed to Wire Haired Fox Terrier
Folder Name from n02116738-African_hunting_dog Changed to African Hunting Dog
Folder Name from n02091467-Norwegian_elkhound Changed to Norwegian Elkhound
Folder Name from n02096294-Australian_terrier Changed to Australian Terrier
Folder Name from n02108422-bull_mastiff Changed to Bull Mastiff
Folder Name from n02096177-cairn Changed to Cairn
Folder Name from n02104365-schipperke Changed to Schipperke
Folder Name from n02101556-clumber Changed to Clumber
Folder Name from n02090721-Irish_wolfhound Changed to Irish Wolfhound
Folder Name from n02110806-basenji Changed to Basenji
Folder Name from n02105251-briard Changed to Briard
Folder Name from n02102040-English_springer Changed to English Springer
Folder Name from n02085620-Chihuahua Changed to Chihuahua
Folder Name from n02110063-malamute Changed to Malamute
Folder Name from n02109525-Saint_Bernard Changed to Saint Bernard
Folder Name from n02107683-Bernese_mountain_dog Changed to Bernese Mountain Dog
Folder Name from n02111129-Leonberg Changed to Leonberg
Folder Name from n02094114-Norfolk_terrier Changed to Norfolk Terrier
Folder Name from n02110627-affenpinscher Changed to Affenpinscher
Folder Name from n02111277-Newfoundland Changed to Newfoundland
Folder Name from n02112350-keeshond Changed to Keeshond
Folder Name from n02106382-Bouvier_des_Flandres Changed to Bouvier Des Flandres
Folder Name from n02093647-Bedlington_terrier Changed to Bedlington Terrier
Folder Name from n02107312-miniature_pinscher Changed to Miniature Pinscher
Folder Name from n02115913-dhole Changed to Dhole
Folder Name from n02111889-Samoyed Changed to Samoyed
Folder Name from n02091032-Italian_greyhound Changed to Italian Greyhound
Folder Name from n02085782-Japanese_spaniel Changed to Japanese Spaniel
Folder Name from n02098286-West_Highland_white_terrier Changed to West Highland White Terrier
Folder Name from n02090379-redbone Changed to Redbone
Folder Name from n02099849-Chesapeake_Bay_retriever Changed to Chesapeake Bay Retriever
Folder Name from n02106662-German_shepherd Changed to German Shepherd
Folder Name from n02105505-komondor Changed to Komondor
Folder Name from n02087046-toy_terrier Changed to Toy Terrier
Folder Name from n02098105-soft-coated_wheaten_terrier Changed to Soft Coated Wheaten Terrier
Folder Name from n02099267-flat-coated_retriever Changed to Flat Coated Retriever
Folder Name from n02104029-kuvasz Changed to Kuvasz
Folder Name from n02096585-Boston_bull Changed to Boston Bull
Folder Name from n02097130-giant_schnauzer Changed to Giant Schnauzer
Folder Name from n02086646-Blenheim_spaniel Changed to Blenheim Spaniel
Folder Name from n02112706-Brabancon_griffon Changed to Brabancon Griffon
Folder Name from n02111500-Great_Pyrenees Changed to Great Pyrenees
Folder Name from n02088466-bloodhound Changed to Bloodhound
Folder Name from n02101006-Gordon_setter Changed to Gordon Setter
Folder Name from n02108089-boxer Changed to Boxer
Folder Name from n02113799-standard_poodle Changed to Standard Poodle
Folder Name from n02086910-papillon Changed to Papillon
Folder Name from n02113712-miniature_poodle Changed to Miniature Poodle
Folder Name from n02095570-Lakeland_terrier Changed to Lakeland Terrier
Folder Name from n02098413-Lhasa Changed to Lhasa
Folder Name from n02106030-collie Changed to Collie
Folder Name from n02092002-Scottish_deerhound Changed to Scottish Deerhound
Folder Name from n02110185-Siberian_husky Changed to Siberian Husky
Folder Name from n02088238-basset Changed to Basset
Folder Name from n02097047-miniature_schnauzer Changed to Miniature Schnauzer
Folder Name from n02108551-Tibetan_mastiff Changed to Tibetan Mastiff
Folder Name from n02105412-kelpie Changed to Kelpie
Folder Name from n02106166-Border_collie Changed to Border Collie
Folder Name from n02102480-Sussex_spaniel Changed to Sussex Spaniel
Folder Name from n02110958-pug Changed to Pug
Folder Name from n02109961-Eskimo_dog Changed to Eskimo Dog
Folder Name from n02096437-Dandie_Dinmont Changed to Dandie Dinmont
Folder Name from n02091831-Saluki Changed to Saluki
Folder Name from n02105056-groenendael Changed to Groenendael
Folder Name from n02113186-Cardigan Changed to Cardigan
Folder Name from n02102973-Irish_water_spaniel Changed to Irish Water Spaniel
Folder Name from n02113978-Mexican_hairless Changed to Mexican Hairless
Folder Name from n02092339-Weimaraner Changed to Weimaraner
Folder Name from n02105855-Shetland_sheepdog Changed to Shetland Sheepdog
Folder Name from n02089973-English_foxhound Changed to English Foxhound
Folder Name from n02102318-cocker_spaniel Changed to Cocker Spaniel
Folder Name from n02097658-silky_terrier Changed to Silky Terrier
Folder Name from n02088632-bluetick Changed to Bluetick
Folder Name from n02091635-otterhound Changed to Otterhound
Folder Name from n02108000-EntleBucher Changed to Entlebucher
Folder Name from n02094258-Norwich_terrier Changed to Norwich Terrier
Folder Name from n02112137-chow Changed to Chow
Folder Name from n02094433-Yorkshire_terrier Changed to Yorkshire Terrier
Folder Name from n02097474-Tibetan_terrier Changed to Tibetan Terrier
Folder Name from n02100583-vizsla Changed to Vizsla
Folder Name from n02097209-standard_schnauzer Changed to Standard Schnauzer
Folder Name from n02096051-Airedale Changed to Airedale
Folder Name from n02091134-whippet Changed to Whippet
Folder Name from n02107908-Appenzeller Changed to Appenzeller
Folder Name from n02105641-Old_English_sheepdog Changed to Old English Sheepdog
Folder Name from n02085936-Maltese_dog Changed to Maltese Dog
Folder Name from n02087394-Rhodesian_ridgeback Changed to Rhodesian Ridgeback
Folder Name from n02108915-French_bulldog Changed to French Bulldog
Folder Name from n02100877-Irish_setter Changed to Irish Setter
Folder Name from n02109047-Great_Dane Changed to Great Dane
Folder Name from n02107574-Greater_Swiss_Mountain_dog Changed to Greater Swiss Mountain Dog
Folder Name from n02086240-Shih-Tzu Changed to Shih Tzu
Folder Name from n02093859-Kerry_blue_terrier Changed to Kerry Blue Terrier
Folder Name from n02086079-Pekinese Changed to Pekinese
Folder Name from n02107142-Doberman Changed to Doberman
Folder Name from n02088364-beagle Changed to Beagle
Folder Name from n02101388-Brittany_spaniel Changed to Brittany Spaniel
Folder Name from n02093754-Border_terrier Changed to Border Terrier
Folder Name from n02089078-black-and-tan_coonhound Changed to Black And Tan Coonhound
Folder Name from n02099712-Labrador_retriever Changed to Labrador Retriever
Folder Name from n02093991-Irish_terrier Changed to Irish Terrier
Folder Name from n02090622-borzoi Changed to Borzoi
Folder Name from n02091244-Ibizan_hound Changed to Ibizan Hound
Folder Name from n02102177-Welsh_springer_spaniel Changed to Welsh Springer Spaniel
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hopefully, this post was helpful in some way !&lt;/p&gt;

</description>
        <pubDate>Tue, 20 Oct 2020 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2020/10/20/changing-folder-name-for-model-training/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2020/10/20/changing-folder-name-for-model-training/</guid>
        
        <category>python</category>
        
        <category>filename</category>
        
        <category>datascience</category>
        
        <category>dog-breed</category>
        
        
      </item>
    
      <item>
        <title>[Spirituality] Contemplating on Great Verses By Yogi Vasistha</title>
        <description>&lt;p&gt;&lt;img src=&quot;http://2.bp.blogspot.com/-rj5tXab7_aA/UdGNLPctzNI/AAAAAAAAAE0/zH3rlohK5rI/s960/Yoga+Vasistha+-+muneshkumarkell-blogspot.jpg&quot; alt=&quot;yogi&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;verse-&quot;&gt;Verse –&lt;/h2&gt;
&lt;p&gt;(I) : When the truth is known, all descriptions cease, and silence alone remains.&lt;/p&gt;

&lt;p&gt;(II) : The mind is purified by persistent contemplation of truth. Only when the mind is totally purified of all conditioning does it regain its utter purity; that pure mind experiences liberation&lt;/p&gt;

&lt;p&gt;(III) : Bondage is none other than the notion of an object. The notions of I and the world are but shadows, not truth. Such notions alone create objects; these objects are neither true nor false. Therefore abandon the notions of I and this and remain established in the truth&lt;/p&gt;

&lt;p&gt;(IV) : It is only when the mind has become devoid of all attachment, when it is not swayed by the pairs of opposites, when it is not attracted by objects and when it is totally independent of all supports, that it is freed from the cage of delusion.&lt;/p&gt;

&lt;p&gt;(V) : Wealth is the mother of evil. Sense-pleasure is the source of pain. Misfortune is the best fortune. Rejection by all is victory. Life, honor and noble qualities blossom and attain fruition in one whose conduct and behavior are good and pleasant, who is devoted to seclusion and who does not crave for the pleasures of the world, which lead to suffering.&lt;/p&gt;

&lt;p&gt;(VI) : When in a pure mind there arise concepts and notions, the world appearance comes into being. But, when the mind gives up the subject-object relationship it has with the world, it is instantly absorbed in the infinite.&lt;/p&gt;

&lt;p&gt;(VII) : When you are free from all concerns about the objects of the world, you will be established in non-dual consciousness, and that is final liberation. Live without being swayed by likes and dislikes, attraction and aversion, without any desires or cravings. Constantly seek to discover the supreme peace.&lt;/p&gt;

&lt;p&gt;(VIII) : One should enjoy the delight that flows from peace. The man whose mind is well-controlled is firmly established in peace. When the heart is thus established in peace, there arises the pure bliss of the Self without delay.&lt;/p&gt;

&lt;p&gt;(IX) : Consciousness free from the limitations of the mind is known as the inner intelligence: it is the essential nature of no-mind. That is the reality, that is supreme consciousness, that is the state known as the supreme self, that is omniscience&lt;/p&gt;

&lt;p&gt;(X) : O mind, abandon this perception of diversity and realize the unreality of your own independence from the infinite consciousness: this is liberation.&lt;/p&gt;

&lt;p&gt;(XI) : Death does not wish to kill one who does not have raga-dvesa (attraction and aversion) nor false notions and mental habits. Death does not wish to kill one who does not suffer from mental illness, who does not entertain desires and hopes which give rise to anxieties and worry, who is not poisoned by greed, whose body and mind are not burnt by the fire of anger and hate, who is not churned and ground by the mill of lust, who is firmly established in the pure awareness of Brahman and the absolute and whose mind is not distracted like a monkey.&lt;/p&gt;

&lt;p&gt;(XII) : In fact, that bliss is inexpressible and indescribable and should not even be called happiness! The mind of the knower of the truth is no-mind: it is pure satva. After living with such no-mind for some time, there arises the state known as turiya-atita (the state beyond the transcendental, or the turiya state).&lt;/p&gt;

&lt;p&gt;(XIII) : The inner light that shines as pure experiencing in all beings, that alone is the self which is indicated by the word I: this is for certain.&lt;/p&gt;

&lt;p&gt;[Vasistha:] “&lt;em&gt;It is wrong perception that sees a bracelet in gold. The mere appearance becomes the cause for such wrong perception. This Maya (unreal appearance) is but a figure of speech, the appearance has the same relation to the supreme self that a wave has to the ocean. When one sees this truth, the appearance ceases to be a delusion. It is on account of ignorance that this long-dream world-appearance appears to be real: thus does the Individual Self come into being. But when the truth is realized, it is seen that all this is the self&lt;/em&gt;.”&lt;/p&gt;

&lt;p&gt;(XIV) : Rama, expand the mind with the mind. Remain at peace within your self, seeing the one infinite being in all. Like the king Bhagiratha you will achieve the impossible if you are able to remain firm in your knowledge of the truth and if you engage yourself in appropriate action in a life characterized by effortless experiencing of the natural course of events.&lt;/p&gt;

&lt;p&gt;[Vasistha:]
“&lt;em&gt;He (King Bhagiratha) approached his guru Tritala and prayed, Lord, how can one put an end to this sorrow and to old age, death and delusion which contribute to repeated birth here?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tritala said: Sorrow ceases, all the bondages are cut and doubts are dispelled when one is fully established in the equanimity of the self for a long time, when the perception of division has ceased and when there is the experience of fullness through the knowledge of that which is to be known. What is to be known?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;It is the self which is pure and which is of the nature of pure consciousness which is omnipresent and eternal.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bhagiratha asked: I know that the self alone is real and the body, etc., are not real. But how is it that it is not perfectly clear to me?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tritala said: Such intellectual knowledge is not knowledge! Unattachment to wife, son and house, equanimity in pleasure and pain, love of solitude, being firmly established in self-knowledge — this is knowledge, all else is ignorance! Only when the egosense is thinned out does this self-knowledge arise.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bhagiratha asked: Since this egosense is firmly established in this body, how can it be uprooted?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tritala replied: By self-effort and by resolutely turning away from the pursuit of pleasure. And by the resolute breaking down of the prison-house of shame (false dignity), etc. If you abandon all this and remain firm, the egosense will vanish and you will realize that you are the supreme being&lt;/em&gt;.”&lt;/p&gt;
</description>
        <pubDate>Wed, 14 Oct 2020 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2020/10/14/contemplating-on-great-verses-by-yogi-vasistha/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2020/10/14/contemplating-on-great-verses-by-yogi-vasistha/</guid>
        
        <category>spirituality</category>
        
        <category>contemplating_series</category>
        
        
      </item>
    
      <item>
        <title>Biggest Dataset on Internet</title>
        <description>&lt;h1 id=&quot;what-are-we-doing-and-why&quot;&gt;What are we doing? And Why?&lt;/h1&gt;
&lt;p&gt;If you are a datascientist or computer vision researcher who always looking for neat image  or any sort of dataset because it’s sometimes so hard to find right dataset for you. In this post, i am sharing some resources which would be super helpful for you. I will also show the right way to download them in case of &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Google Open Image Dataset&lt;/code&gt;&lt;/strong&gt;. What right way you may ask? It’s simply a python script which will do the job for you. You don’t need to rush in anyway.&lt;/p&gt;

&lt;h1 id=&quot;open-images-dataset-v6&quot;&gt;Open Images Dataset V6&lt;/h1&gt;

&lt;p&gt;When i was starting out from scratch. It was quite difficult to know from where to download the dataset. Or if that dataset right for you. Open Images Dataset V6 by Google is an amazing source to download the data.&lt;/p&gt;

&lt;p&gt;You can find it &lt;a href=&quot;https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&amp;amp;type=detection&amp;amp;c=%2Fm%2F0pg52&quot;&gt;here&lt;/a&gt;. There will be plethora of categories in the dropdown menu. It would look something like this.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/ygqVGYWV&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/1tXff8QN/googleos.png&quot; alt=&quot;googleos.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;brief&quot;&gt;Brief&lt;/h2&gt;

&lt;p&gt;As you can see, the category for this tutorial i have chosen is taxi. You may chose anything else, Ofcourse!. There are several filters on the top of red bar in the website which is important to know about. Like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Subset : (Train, Validation)&lt;/li&gt;
  &lt;li&gt;Type: (Detection, Segementation)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Subset is only to show you the content which will be downloaded if you download train or validation filtered data.&lt;/p&gt;

&lt;p&gt;Type is crucial, it will give you whatever type of problem deal with. For example, for this example we have used detection. So the images we are getting is bounding boxes. If you switch it to, segmentation you get segemented image. As simple as that.&lt;/p&gt;

&lt;h2 id=&quot;how-to-download&quot;&gt;How to download?&lt;/h2&gt;

&lt;p&gt;It’s quite difficult to ambigous to download from the website. But fortunately we have tool which makes it easy to one liner!&lt;/p&gt;

&lt;p&gt;We use a tool name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OIDv4_ToolKit&lt;/code&gt; available on &lt;a href=&quot;https://github.com/theAIGuysCode/OIDv4_ToolKit&quot;&gt;github&lt;/a&gt;. It makes is fairly easy to download images.&lt;/p&gt;

&lt;p&gt;Cloning the github repo.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;📝 Note: If you are running the command in a terminal. Omit “!” .&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt; git clone https://github.com/theAIGuysCode/OIDv4_ToolKit.git 
Cloning into 'OIDv4_ToolKit'...
remote: Enumerating objects: 444, done.
remote: Total 444 (delta 0), reused 0 (delta 0), pack-reused 444
Receiving objects: 100% (444/444), 34.09 MiB | 35.95 MiB/s, done.
Resolving deltas: 100% (157/157), done.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Moving inside directory and extracting some files. You don’t need to bother much about this, just copy paste and run on your machine.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt; cd OIDv4_ToolKit/
&amp;gt; curl &quot;https://d1vvhvl2y92vvt.cloudfront.net/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot;
&amp;gt; unzip awscliv2.zip
&amp;gt; sudo ./aws/install
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now, here comes the magic. One command where you specify :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;class&lt;/code&gt; : In our case, we will download &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Taxi&lt;/code&gt; images. You can download multiple classes by just typing class one after another.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;type_csv&lt;/code&gt;: Do you want to download train data? Validation data?&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;limit&lt;/code&gt;: How many images we want to download? I am downloading 100 as an example.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt; python main.py downloader --classes Taxi --type_csv train --limit 100
.'   `.|_   _||_   _ `.         | |  | |   
		/  .-.  \ | |    | | `. \ _   __ | |__| |_  
		| |   | | | |    | |  | |[ \ [  ]|____   _| 
		\  `-'  /_| |_  _| |_.' / \ \/ /     _| |_  
		 `.___.'|_____||______.'   \__/     |_____|
	

             _____                    _                 _             
            (____ \                  | |               | |            
             _   \ \ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ 
            | |   | / _ \| | | |  _ \| |/ _ \ / _  |/ || |/ _  )/ ___)
            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    
            |_____/ \___/ \____|_| |_|_|\___/ \_||_|\____|\____)_|    
                                                          
        
    [INFO] | Downloading Taxi.
   [ERROR] | Missing the class-descriptions-boxable.csv file.
[DOWNLOAD] | Do you want to download the missing file? [Y/n] Y
...145%, 0 MB, 31097 KB/s, 0 seconds passed
[DOWNLOAD] | File class-descriptions-boxable.csv downloaded into OID/csv_folder/class-descriptions-boxable.csv.
   [ERROR] | Missing the train-annotations-bbox.csv file.
[DOWNLOAD] | Do you want to download the missing file? [Y/n] Y
...100%, 1138 MB, 33727 KB/s, 34 seconds passed
[DOWNLOAD] | File train-annotations-bbox.csv downloaded into OID/csv_folder/train-annotations-bbox.csv.

Taxi
    [INFO] | Downloading train images.
    [INFO] | [INFO] Found 1434 online images for train.
    [INFO] | Limiting to 100 images.
    [INFO] | Download of 100 images in train.
100% 100/100 [02:08&amp;lt;00:00,  1.28s/it]
    [INFO] | Done!
    [INFO] | Creating labels for Taxi of train.
    [INFO] | Labels creation completed.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h1 id=&quot;sample-image-from-data&quot;&gt;Sample image from data&lt;/h1&gt;

&lt;p&gt;Now it has been downloaded. Dataset will be downloaded in the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/OID/Dataset/train/&lt;/code&gt;
Let’s see the sample image?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt; from PIL import Image
&amp;gt; Image.open('/content/OIDv4_ToolKit/OID/Dataset/train/Taxi/23274c285653cc1c.jpg')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://postimg.cc/Cn5nhxCW&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/3NCXZykJ/car.png&quot; alt=&quot;car.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So it looks pretty good! Remember i have only downloaded 100 images. You can download with any limit. If its available on dataset. It will be downloaded. We also have bounding boxes in labels folder.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚡ Tip: csv file have been downloaded in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;csv_folder&lt;/code&gt;. You can use it as pandas dataframe for more flexible usage of data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;open-public-datasets&quot;&gt;Open Public datasets&lt;/h1&gt;
&lt;p&gt;As a data scientist, you dont always deal with image dataset. So, this &lt;a href=&quot;https://github.com/hiteshhedwig/awesome-public-datasets&quot;&gt;Github&lt;/a&gt; Repo got very detailed list of every dataset for gamut of professions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://postimg.cc/YvzrPwYg&quot;&gt;&lt;img src=&quot;https://i.postimg.cc/0yLzbkNt/dataset.png&quot; alt=&quot;dataset.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚡ Tip: If you are new to github. You can fork it and contribute to it as well.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;amazon-google-microsoft-public-dataset&quot;&gt;Amazon, Google, Microsoft Public Dataset&lt;/h1&gt;

&lt;p&gt;Waait.. we just discussed google dataset a while ago. That was especially for image based problems. Incase you want to research for the data yourself that you struggling to find. The &lt;a href=&quot;https://datasetsearch.research.google.com/&quot;&gt;Google Dataset Search engine&lt;/a&gt; will help you to research more about it.&lt;/p&gt;

&lt;p&gt;Like Google, &lt;a href=&quot;https://registry.opendata.aws/&quot;&gt;Amazon&lt;/a&gt; also have some public dataset to help you with your research.&lt;/p&gt;

&lt;p&gt;And so do, &lt;a href=&quot;https://azure.microsoft.com/en-in/services/open-datasets/catalog/&quot;&gt;Microsoft&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;Datasets we have discussed so far. They will definitely provide the edge you looking for (if you look correctly). They are almost all you need. Although there are sites like kaggle, datatruks but as we have mentioned google dataset engine. It automatically directs you to the sites.&lt;/p&gt;

</description>
        <pubDate>Mon, 12 Oct 2020 00:00:00 +0530</pubDate>
        <link>https://hiteshhedwig.github.io/hedwig_explains/2020/10/12/biggest-dataset-on-internet/</link>
        <guid isPermaLink="true">https://hiteshhedwig.github.io/hedwig_explains/2020/10/12/biggest-dataset-on-internet/</guid>
        
        <category>dataset</category>
        
        <category>opensource</category>
        
        
      </item>
    
  </channel>
</rss>
